\section{Approach} \label{sec:Approach}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Discuss:
%%     your approach to the problem
%%   
%%     why its good or better
%%   
%%     details of solution (no results)
%%   
%%     where did your data come from
%%   
%%     where did your model(s) come from
%%   
%%     algorithms to be used
%%   
%%     how will you explore the parameter space
%% 
%%     analysis to be done on data
%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In this section we will first present reasons \emph{why} the aforementioned
programming methods will not work well, followed by motivation to use the
methods we did. After these rather nebulous postulations, we will discuss
\emph{our} particular implementation, starting from the ground up. Finally
we will explain what data we collected and why we believed they were important.

\subsection{Ineffective Theory}

\subsubsection{Traditional Procedural Programming}
Consider a stream or river. (Disclaimer: this is a massive simplification of
one of nature's most majestic features.) Despite potentially massive deluges, 
the model is still fairly predictable: water starts at the source and flows 
down to the mouth. If we use the analogy of the water in a river being data 
flow in a program, we can say that if the water reaches the river delta, it
has successfully arrived at its goal.

But what of lakes and seas? There is still current in the water, but when
can one say that it has reached its ``goal?'' Where did it start in the first
place?

Likewise, when has our agent reached its goal? Perhaps we could say it is 
when the agent has consumed all the food in Flatworld, but was the agent 
efficient in doing so? Let us consider the potential program flow that
would be required for modeling an agent's simple decision whether to eat one
object it sees compared with another:

\begin{verbatim}
  if object0 is food
    if object0 is only object in sight
      eat object0
    else if object1 is food
      if object1 is closer than object0
        eat object1 
      else
        eat object0
  rinse and repeat
\end{verbatim}

This could be rearranged in multiple ways to produce the same behavior,
but the result would be the same; even this simple action is quite cumbersome
to implement.

\subsubsection{Declarative (e.g. Logic) Programming}
Say we look at this same decision and action from a declarative programming
paradigm, and specifically implementing first order predicate logic.
\begin{equation} \label{eq:logic1}
  \large
  \begin{aligned}
    \exists\ obj_i \in &\{Food\ \wedge\ \forall obj_n \in Food, \text{where }
    n \neq i, \neg closer(obj_n,obj_i)\}
  \end{aligned}
\end{equation}

Equation \eqref{eq:logic1}, like the code above, describes whether or 
not to eat an object. If \eqref{eq:logic1} evaluates as $True$, then the agent 
will be told to eat the object. Now we have introduced other problems! For 
instance, we must populate $Food$ \emph{and} parse 
through it every time the agent considers an object. 

\subsubsection{As if it Wasn't Bad Enough Already...}
These two approaches have already proven to be ineffective in accomplishing
our goal, but then we must enforce a particularly harsh and realistic constraint: 
\textbf{there is no oracle in Flatworld}. That is, the agent must acquire
important knowledge on its own. Object distance and type are not simply
dictated to the agent.\footnote{Although later we shall reveal that we did
do this, and discuss why we could justify this.} Even though creating and
scanning the set $Food$ can be done in $O(n)$ time, our agent has no working
knowledge of where all the food objects are, or even how many exist
in Flatworld.


\subsection{Effective Theory}
It may not come as a surprise that we decided to approach this problem using
neural networks. We were perhaps a little harsh on our discussion of 
procedural programming. First of all, Flatland itself is implemented in C,
along with the agent. Secondly, we can still apply the mathematics involved
in neural networks with procedural languages like C. Rather than using a
rigid conditional structure, however, we programmed our agent's brains to
be able to adjust themselves based upon the situations the agent encounters.

\subsection{Our Implementation}
For all of our runs we used a shim\footnote{See section~\ref{sec:Ack}.} that 
sat between Flatworld's C code and our Python code. This served two purposes:
since we already had a working object to describe neurons in Python we could
recycle it for this project; that, and we only had to compile the C code once.
This was possible because the shim was compiled with a set of hard-coded
python function calls. So to modify the behavior of a brain, we
only needed to change the python code called by the shim.
Given the number of minute changes we made to each incarnation of our brains,
avoiding recompiling every time allowed for immediate results.

